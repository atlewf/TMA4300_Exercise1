---
title: "TMA4300 Project 1"
author: "Atle Wiig-Fisketj√∏n"
date: "17.04.2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem A

## 1

```{r}
#generates n samples from exp distribution with rate parameter lambda
exponential <- function(lambda, n) { 
  return (-1/lambda * log(runif(n)))
}
```

For the exponential distribution we have mean $E[X] = \frac{1}{\lambda}$ and variance $Var[X] = \frac{1}{\lambda^2}$. We compare the mean and variance of the sample with the theoretical values.
```{r}
lambda = 1/3
x = exponential(1/3, 1000000)
mu = mean(x)
sigma = var(x)
theor_mean = 1/lambda
theor_var = 1/lambda^2

cat("Sample mean: ", mu, ", Theoretical mean: ", theor_mean, "\n")
cat("Sample variance: ", sigma, ", Theoretical variance: ", theor_var, "\n")

```
Hence the function is implemented correctly.

## 2

### a)
The probability density function $g(x)$ is defined as 

$$g(x) = \begin{cases} c x^{\alpha - 1}, \quad 0 < x < 1, \\
c e^{-x}, \quad 1 \leq x, \\
0, \quad \text{otherwise},
\end{cases} $$
for a normalizing constant $c$ and $\alpha \in (0,1)$. First compute the normalizing constant:
$$
\int_{-\infty}^{\infty} g(x)dx = \int_0^1 cx^{\alpha-1}dx + \int_1^{\infty}ce^{-x}dx = c(\frac{1}{\alpha} + \frac{1}{e})
$$
Setting this equal to $1$ and solving for c gives $c = \frac{\alpha e}{\alpha + e}$.
Finding the cumulative distribution:
$$
G(x) = \int_{-\infty}^xg(t)dt = \begin{cases} \int_0^1 c t^{\alpha-1}dt = \frac{c}{\alpha}x^{\alpha}, \quad & 0 \leq x \leq 1 \\
G(1) + \int_1^x ce^{-t}dt = \frac{c}{\alpha} + c(\frac{1}{e}- e^{-x}) = 1 -ce^{-x}, \quad & 1 \leq x \\
0, \quad & \text{otherwise}
\end{cases}
$$
Take the inverse to obtain
$$
G^{-1}(y) = \begin{cases} (\frac{\alpha}{c} y)^{1/\alpha}, \quad &0 \leq y< \frac{c}{\alpha}, \\
\ln(\frac{c}{1-y}), \quad &\frac{c}{\alpha} \leq y \leq 1,
\end{cases}
$$
where the domains where found by solving $y=g(x)$ for $x=0, x=1$, and $\lim_{x \to \infty}$ as we see that the function is monotone. 

### b)
The following function generates n samples from $g(x)$, using the method of inversion sampling.
```{r}
G_inverse <- function (n, alpha) { #generate n samples from g(x) using inversion sampling
  c <-  alpha *exp(1) /(alpha+exp(1))
  u <- runif(n)
  res <- rep(0, n)
  res[u<c/alpha] <- ((alpha/c)*u[u<c/alpha])**(1/alpha)
  res[u>=c/alpha] <- log(c/(1-u[u>=c/alpha]))
  return (res)
}
```
To check the implementation we first find the theoretical mean and variance for $g(x)$.

\begin{align*}
E[X] &= \int_{-\infty}^{\infty} x g(x) dx = \int_0^1 cx^{\alpha}dx + \int_1^{\infty}xce^{-x}dx = \frac{c}{\alpha+1} + \frac{2c}{e} \\
Var[X] &= E[X^2] - E[X]^2 = \frac{c}{\alpha+2} + \frac{5c}{e} - \left( \frac{c}{\alpha+1} + \frac{2c}{e}\right )^2
\end{align*}

Now we compare the mean and variance of the sample with theoretical values:
```{r}
alpha = 0.25
n = 1000000
x_vals = G_inverse(n, alpha)
c = alpha*exp(1)/(alpha + exp(1))


theor_mean = c/(alpha+1) + 2*c/exp(1)
theor_var = c/(alpha+2) + 5*c/exp(1) - (theor_mean)^2
sample_mean = mean(x_vals)
sample_var = var(x_vals)

cat("Sample mean: ", sample_mean, ", Theoretical mean: ", theor_mean, "\n")
cat("Sample variance: ", sample_var, ", Theoretical variance: ", theor_var, "\n")
```
Hence the implementation is correct.

## 3
The Box-Muller algorithm generate samples from the standard normal distribution, and is implemented as follows:
```{r}
normal <- function(n){ #generate n samples from standard normal distribution
  x1 <- 2*pi*runif(1/2 * n) #assumes n to be an even number
  x2 <- exponential(1/2, n/2)
  y1 <- sqrt(x2) * cos(x1) #1/2 of the generated samples
  y2 <- sqrt(x2) * sin(x1) #other half of samples
  return (c(y1, y2))
}
```
The standard normal distribution has theoretical mean $\mu=0$ and variance $\sigma^2 =1$. We check our implementation:
```{r}
n = 1000000
sample_normal = normal(n)
sample_mean = mean(sample_normal)
sample_var = var(sample_normal)

cat("Sample mean: ", sample_mean, ", Theoretical mean: ", 0, "\n")
cat("Sample variance: ", sample_var, ", Theoretical variance: ", 1, "\n")

```
Hence the implementation is correct.


## 4
To sample from the d-variate normal distribution, we use the property of linear transformation; for $x \sim \mathcal{N}_d(\mu, \Sigma)$, ${\bf A} \in \mathbb{R}^{r \times d}$, ${\bf b}\in \mathbb{R}^r$, we have ${\bf y} = {\bf A x} + {\bf b} \sim \mathcal{N}_r({\bf A \mu} + {\bf b},  {\bf A \Sigma A^{\top}})$. The function is then:

```{r}
#generate one sample from the d-variate normal distribution
multi_normal  <-function(d, mean, cov) { 
  A <- chol(cov) #computing cholesky factorization of cov.matrix
  x <- normal(d)
  y <- mean + t(A) %*% x #need to transpose the chol. fact.
  return (y)
}
```
Checking the implementation:

```{r}
#note that the algorithm will only work for even numbers for d, beacause of the implementation of "normal(n)"
d = 2  
n = 100000
mean = c(-3, 1) #mean vec
cov = matrix(c(2, 1, 1, 4), ncol=2 ,nrow=2) #cov matrix
sample = matrix(NA, ncol = d, nrow = n) #matrix to store values in
for (i in 1:n){
  sample[i,] = multi_normal(d, mean, cov) #n samples, each sample stored as row in matrix
}
sample_mean=colMeans(sample)
sample_cov = cov(sample)
cat("Sample mean: ", mean, ", Theoretical mean: ", mean,  "\n")
cat("Sample variance: ", cov(sample), ", Theoretical variance: ", cov, "\n")
```

# Problem B

## 1

We consider the Gamma distribution with parameters $\alpha \in (0,1)$  and $\beta = 1$, i.e.
$$ 
f(x) = \begin{cases}
\frac{1}{\Gamma(\alpha)}x^{\alpha-1}e^{-x}, \quad & 0<x,\\
0, \quad &\text{otherwise}. \\
\end{cases} 
$$
To generate samples from this distribution we will use rejection sampling, using $g(x)$ from A.2 as proposal distribution. 

#### a)
The acceptance probability is in general
$$
P\bigg(U\leq \frac{1}{c}\frac{f(x)}{g(x)}\bigg) = \int_{-\infty}^{\infty} \frac{f(x)}{c g(x)} g(x)dx = c^{-1}.
$$
We want to maximize the acceptance probability, i.e. minimizing $c$. Do this by setting
$$ 
c = \underset{x}{\sup} \frac{f(x)}{g(x)} = \underset{x}{\sup} \begin{cases}
\frac{e^{-x}(\frac{1}{\alpha} + \frac{1}{e})}{\Gamma(\alpha)}, \quad 0 < x < 1 \\
\frac{x^{\alpha - 1} (\frac{1}{\alpha} + \frac{1}{e})}{\Gamma(\alpha)}, \quad 1 \leq x,
\end{cases}
$$
which yields $c = \frac{(\frac{1}{\alpha} + \frac{1}{e})}{\Gamma(\alpha)}$.

### b)
First we need to implement the function $g(x)$.
```{r}
g <- function(x, alpha) { #takes a vector x and returns a vector of computed values g(x)
  c <-  alpha *exp(1) /(alpha+exp(1))
  res <- rep(0, length(x))
  res[(x<1) & (x>0)] <- c * x[x<1]**(alpha-1)
  res[x>=1] <- c*exp(-x[x>=1])
  return (res)
}
```

Now we implement the rejection sampling algorithm. 
```{r}
gamma_distr <- function(alpha, n=1){ #draw n samples from gamma distribution
  sampled <- 0 #number of accepted samples
  c <- (1/alpha + 1/exp(1))/gamma(alpha) #actual gamma function
  res <- vector() #vector for storing sampled values
  while (sampled < n){
    x <- G_inverse(n, alpha)
    u <- runif(n)
    acceptance <- 1/c * dgamma(x, alpha, rate=1)/g(x, alpha)
    res <- append(res, x[u <= acceptance])
    sampled <- sampled + sum(u <= acceptance) #update number of accepted samples
  }
  
  return (head(res, n)) #only want first n samples (there may be drawn redundant samples)
}
```
Check implementation by comparing the mean and variance to the theoretical ones. For a gamma distribution with parameters shape $\alpha$ and rate $\beta$, we have mean $E[X] = \frac{\alpha}{\beta}$ and variance $Var[X] =\frac{\alpha}{\beta^2}$. Here this corresponds to both mean and variance equal to $\alpha$.

```{r}
n = 1000000
alpha = 0.75
samples = gamma_distr(alpha, n)

cat("Sample mean: ", mean(samples), ", Theoretical mean: ", alpha, "\n")
cat("Sample variance: ", var(samples), ", Theoretical variance: ", alpha, "\n")
```
So the implementation is correct.

## 2

### a) 
We start by defining the area $C_f$,
$$
C_f = \left\{ (x_1, x_2): 0 \leq x_1 \leq \sqrt{f^*\left(\frac{x_2}{x_1}\right)} \right\} \quad \text{where} \quad f^*(x) = \begin{cases}
x^{\alpha-1}e^{-x}, \quad &0 < x, \\
0, \quad &\text{otherwise},
\end{cases}
$$
and the constants $a$, $b_-$ and $b_+$,
$$ a = \sqrt{\sup_x f^* (x)}, \quad b_{+} = \sqrt{ \sup_{x\geq 0} (x^2 f^* (x))} , \quad b_{\_} = - \sqrt{\sup_{x \leq 0} (x^2 f^* (x))},$$
so that $C_f \subset [0,a] \times [b_-, b_+]$.
Finding the values:
\begin{align*}
(f^\ast(x))' &= 0 \Leftrightarrow (\alpha-1)x^{\alpha-2}e^{-x} - x^{\alpha-1}e^{-x} = 0  \Leftrightarrow (\alpha-1-x)x^{\alpha-2} = 0 \implies x = \alpha - 1 \\
a &= \sqrt{(\alpha - 1)^{\alpha - 1}e^{1-\alpha}}
\end{align*}
\begin{align*}
(x^2 f^\ast(x))' &= 0 \Leftrightarrow (\alpha+1)x^{\alpha-2}e^{-x} - x^{\alpha+1}e^{-x} = 0  \Leftrightarrow (\alpha+1-x)x^{\alpha-2} = 0 \implies x = \alpha + 1\\
b_{+} &= \sqrt{(\alpha + 1)^{\alpha + 1}e^{-\alpha - 1}} \\
\\
x^2f^\ast(x) &= 0, \quad x \leq 0 \\
b_{-} &= 0
\end{align*}

# Problem C

## 1

Assume $z_k\sim gamma(\alpha_k,1)$ for $k=1,\ldots,K$ independently, and define $x_k = z_k/(z_1+\dots+z_K)$ for $k=1,\ldots,K$. We want to show that then $x= (x_1,\ldots,x_K)$ has a Dirichlet distribution. We start by using the change-of-variables formula to transform $(z_1,\ldots,z_K)$ to $(x_1,\ldots,x_{K-1},v)$, where $v = z_1+\dots+z_K$. The inverse transformation is $z_k = vx_k$ for  $k = 1,\ldots,K$, with the Jacobi determinant

$$
\lvert J \rvert 
= \begin{vmatrix}\frac{\partial z_1}{\partial x_1}  & \dots & \frac{\partial z_1}{\partial x_{K-1}} & \frac{\partial z_1}{\partial v}\\ \vdots &  \ddots & \vdots & \vdots \\ \frac{\partial z_K}{\partial x_1} & \dots & \frac{\partial z_K}{\partial x_{K-1}}&\frac{\partial z_K}{\partial v} \end{vmatrix}
= \begin{vmatrix}v&0  & \dots & 0 & x_1\\ 0&v&\dots & 0 & x_2\\\vdots &\vdots&  \ddots & \vdots & \vdots \\0 & 0& \dots&v&x_{K-1}\\ -v &-v& \dots & -v&1-\sum_{k=1}^{K-1}x_k  \end{vmatrix}
= \begin{vmatrix}v&0  & \dots & 0 & x_1\\ 0&v&\dots & 0 & x_2\\\vdots &\vdots&  \ddots & \vdots & \vdots \\0 & 0& \dots&v&x_{K-1}\\ 0 &0& \dots & 0&1\end{vmatrix}
= v^{K-1}.
$$
As $z_k$ for $k = 1,\ldots,K$ are independent, the joint distribution is
$$
F_{z_1,\ldots,z_K}(z_1,\ldots,z_K) = \prod_{k=1}^K \frac{1}{\Gamma(\alpha_k)}z_k^{\alpha_k-1}e^{-z_k}=\prod_{k=1}^K \bigg(\frac{1}{\Gamma(\alpha_k)}z_k^{\alpha_k-1}\bigg)e^{-\sum_{k=1}^K z_k}.
$$
We perform the transformation with $z_k = vx_k, k = 1,\ldots,K-1$ and $v = z_1+\dots+z_K$ to get

\begin{align*}
F_{x_1,\ldots,x_{K-1},v}(x_1,\ldots,x_{K-1},v)
&= \left( \prod_{k=1}^{K-1} \frac{1}{\Gamma(\alpha_k)}(vx_k)^{\alpha_k-1}\right) \cdot\frac{1}{\Gamma(\alpha_K)}\left(v(1-\sum_{k=1}^{K-1}x_k)\right)^{\alpha_K-1}e^{-v}\cdot v^{K-1}\\
&= \left(\prod_{k=1}^{K}\frac{1}{\Gamma(\alpha_k)}\right)v^{\left(\sum_{k=1}^K\alpha_k\right)-1}\left(\prod_{k=1}^{K-1}x_k^{\alpha_k-1}\right)\left(1-\sum_{k=1}^{K-1}x_k\right)^{\alpha_K-1}e^{-v}.
\end{align*}

To obtain the marginal distribution we integrate out $v$:

\begin{align*}
F_{x_1,\ldots,x_{K-1}}(x_1,\ldots,x_{K-1}) &= \int_0^\infty F_{x_1,\ldots,x_{K-1},v}(x_1,\ldots,x_{K-1},v) dv \\
&= \left(\prod_{k=1}^{K}\frac{1}{\Gamma(\alpha_k)}\right)\left(\prod_{k=1}^{K-1}x_k^{\alpha_k-1}\right)\left(1-\sum_{k=1}^{K-1}x_k\right)^{\alpha_K-1} \int_0^\infty v^{\left(\sum_{k=1}^K\alpha_k\right)-1}e^{-v}dv \\
&= \left(\prod_{k=1}^{K}\frac{1}{\Gamma(\alpha_k)}\right)\left(\prod_{k=1}^{K-1}x_k^{\alpha_k-1}\right)\left(1-\sum_{k=1}^{K-1}x_k\right)^{\alpha_K-1} \Gamma\left(\sum_{k=1}^K\alpha_k\right) \\
&= \frac{\Gamma\left(\sum_{k=1}^K\alpha_k\right)}{\prod_{k=1}^{K}\Gamma(\alpha_k)}\bigg(\prod_{k=1}^{K-1}x_k^{\alpha_k-1}\bigg)\bigg(1-\sum_{k=1}^{K-1}x_k\bigg)^{\alpha_K-1},
\end{align*}


where we use the definition of the gamma function, and the fact that $v$ is a sum of Gamma variables and therefore has domain from zero to infinity. Hence $x=(x_1,\ldots,x_K)$ has a Dirichlet distribution with $\alpha = (\alpha_1, \ldots, \alpha_K).$

## 2
The following function assumes that $\alpha_k \in (0,1) \quad \forall k$, as the gamma_distr function only accepts $\alpha$-values in this domain. It generates one realization from the Dirichlet distribution with parameter vector $\alpha = (\alpha_1, \ldots, \alpha_K)$.
```{r}
dirichlet <- function(alpha){ #input vector alpha of length k, return one sample
  z <- sapply(alpha, gamma_distr) #for all k = 1,...,K, draw one sample from gamma distr
  return (z / sum(z)) #return the vector x = z/sum(z), with Dirichlet distr
}
```
For a Dirichlet distribution, we have theoretical mean $E[X_i] = \frac{\alpha_k}{\sum_{k=1}^K \alpha_k}$ and variance $Var[X_i] = \frac{\tilde \alpha_i(1-\tilde \alpha_i)}{\alpha_0+1}$, where $\tilde \alpha_i = \frac{\alpha_i}{\sum_{k=1}^K \alpha_k}$ and $\alpha_0 = \sum_{i=1}^K \alpha_i$. Compare the sample mean and variance with theoretical ones:
```{r}
k = 5
n = 10000
alpha = runif(k) #draw alpha-values in (0,1)
samples = matrix(0, ncol=k, nrow=n)
for (i in 1:n){
  samples[i,] = dirichlet(alpha)
}
mean = alpha/sum(alpha)
cat("Sample mean: ", colMeans(samples), "\n")
cat("Theoretical mean: ", mean, "\n")
cat("Sample variance: ", apply(samples, 2, var), "\n")
cat("Theoretical variance: ", mean*(1-mean)/(sum(alpha)+1), "\n")
```
Hence the implementation is correct.

# Problem D

## 1

The multinomial mass function is given by $f({\bf y} \lvert \theta) \propto (2+\theta)^{y_1}(1-\theta)^{y_2 + y_3}\theta^{y_4}$. We want to sample from this function, using a rejection sampling algorithm with $\mathcal{U}(0,1)$ as proposal density. First use the given data for $y_1, \ldots, y_4$ to implement f.
```{r}
f <- function(theta){
  return((2+theta)^125*(1-theta)^(20+18)*theta^34)
}
```
Now implement the rejection sampling algorithm:
```{r}
rejection_sample <- function(n){
 
  #Find optimal c in f(x)/c g(x). Know that unif(0,1) has g(x) = 1, so c = max(f(x))
  c <- optimize(f, c(0,1), maximum=TRUE)$objective
  
  sampled <- 0
  res <- vector()
  
  while (sampled < n){
    x <- runif(n)
    u <- runif(n)
    acceptance <- 1/c * f(x)/1 # g(x)=1
    res <- append(res, x[u<=acceptance])
    sampled <- sampled + sum(u<= acceptance) #update number of accepted samples
  }
  return (head(res, n))
}
```

## 2
We can estimate the posterior mean using Monte-Carlo integration. In general we are interested in $\mu = E[h(x)], \quad x \sim f(x)$, which has the analytical solution $\mu = \int_{-\infty}^{\infty}h(x)f(x)dx$. The Monte-Carlo estimate is given by $\hat\mu = \frac{1}{N} \sum_{i=1}^N h(x_i)$. For this problem we have estimate $\hat\mu = \frac{1}{M} \sum_{i=1}^{M} x_i$, and analytical solution $\mu = \int_{-\infty}^{\infty}\theta f(\theta \lvert y)d\theta$, where $f(\theta \lvert y)$ represents the normalized density function. We therefore need to compute the normalizing constant: 

```{r}
normalizing <- integrate(f, 0, 1)$value #compute normalizing constant
f_normalized <-function(x){return (f(x)/(normalizing))}
g <- function(x) {return (x*f_normalized(x))} # integrand in analytical solution for E[X]
M = 10000
samples <- rejection_sample(M)
theor_mean = integrate(g,0,1)$value
cat("Sample mean: ", mean(samples), ", Theoretical mean: ", theor_mean, "\n")
```
The sample mean coincide with the theoretical mean. We can also draw a histogram of the samples.
```{r, warning=FALSE}
library(ggplot2)
data <- data.frame(theo=seq(0,1,length.out=M), value=samples)
ggplot(data, aes(x = value)) + geom_histogram(aes(y = ..density.., colour = "Sampled"),
  binwidth=0.001) + stat_function(fun=f_normalized, geom = "line",size=1.6,
  aes(colour="Analytical")) + xlim(0,1) + geom_segment(x = mean(samples),
  xend = mean(samples), y = 0, yend = 10, aes(colour = "Mean"),
  size = 1.2, linetype = "solid") + labs(x=expression(theta), y='Density')
```

Hence the implementation of the algorithm seem to be correct.